# model
model_name: meta-llama/Meta-Llama-3-8B-Instruct
quantize: 4bit

# dataset
data_path: /path/file
remove_wiki: True
train_language: ALL
dev_language: HU
test_language: HU
load_amr3: True
gold_data_amount: 50000
synthetic_data: /path/file
synthetic_data_amount: 5000
frame_arg_descr: /path/file


# training
num_train_epochs: 1
output_dir: /path

learning_rate: 0.0005
weight_decay: 0.01
logging_steps: 10
eval_steps: 10
save_steps: 10 # same as eval_steps
warmup_steps: 1500
max_grad_norm: 0.3
save_total_limit: 1
batch_size: 4
gradient_accumulation_steps: 4
group_by_length: True

# lora
use_lora: True
use_rslora: False
lora_rank: 16
lora_alpha: 32
lora_dropout: 0.05


max_seq_length: 8192
patience: 20


generate_max_length: 1024