model_checkpoint: google/mt5-small
max_input_length: 784
max_output_length: 512

# dataset
data_path: /path/file
remove_wiki: True
train_language: ALL
dev_language: HU
test_language: HU
synthetic_data: /path/file
synthetic_data_amount: 5000
frame_arg_descr: /path/file

# training
output_dir: /path

num_train_epochs: 1
patience: 10
save_total_limit: 3
learning_rate: 0.00005
batch_size: 16
gradient_accumulation_steps: 1
warmup_ratio: 0.03
weight_decay: 0.01
valid_steps: 5000 # same as save_checkpoint_steps
logging_steps: 500
fp16: False
compute_training_metrics: False

# only used when training aya-101
quantize: 4bit
lora_rank: 16
lora_alpha: 16
lora_dropout: 0.05



# predict
prediction_file: None
max_predict_length: 512
num_beams: 5
length_penalty: 2
no_repeat_ngram_size: 2
encoder_no_repeat_ngram_size: 3
generate_early_stopping: True